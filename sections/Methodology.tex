\section{Methodology}

This section describes the systematic definition and approach used to analyze the Covertype dataset, prepare it for machine learning, and evaluate model performance. We organized our methodology into two main phases: data analysis and preprocessing, followed by class imbalance handling and model selection, and ended with model evaluation.

\subsection{Data Analysis and Preprocessing}

\paragraph{Variance Distribution Analysis}

\textbf{Definition} Variance measures how much a feature's values spread out from their mean. For a feature $x$ with $n$ samples, variance is calculated as:

$$\text{Variance} = \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2$$

where $\mu$ is the mean of the feature.

We calculated variance for all features and summarized the distribution using:
\begin{itemize}
    \item Mean: Average variance across features
    \item Standard Deviation: Spread of variance values
    \item Minimum: Lowest variance (identifies near-constant features)
    \item 25th, 50th, 75th Percentiles: Distribution quartiles
    \item Maximum: Highest variance (identifies highly variable features)
\end{itemize}

\textbf{Why:} Variance analysis helps us understand feature informativeness and scale differences. Features with very low variance provide little discriminative information, while features with vastly different variances may require scaling for certain algorithms. This analysis directly informs our decision about whether feature scaling is necessary.

\paragraph{Correlation Analysis}

\textbf{Definition:} Correlation measures the linear relationship between two features. We computed the Pearson correlation coefficient for all pairs of continuous features:

$$r_{xy} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}$$

where $r_{xy} \in [-1, 1]$:
\begin{itemize}
    \item $r = 1$: Perfect positive correlation
    \item $r = 0$: No linear correlation
    \item $r = -1$: Perfect negative correlation
\end{itemize}

\textbf{Effectiveness:} Correlation analysis reveals:
\begin{itemize}
    \item Feature redundancy (highly correlated features may provide similar information)
    \item Multicollinearity issues that could affect certain models
    \item Domain relationships between variables
    \item Potential feature engineering opportunities
\end{itemize}

Strong correlations suggest that some features capture related information, which affects model selection and interpretation.

\paragraph{Statistical Distribution Analysis}

\textbf{Definition:} For each continuous feature, we calculated comprehensive descriptive statistics:

\begin{itemize}
    \item \textbf{Mean ($\mu$)}: $\mu = \frac{1}{n}\sum_{i=1}^{n} x_i$
    \item \textbf{Standard Deviation ($\sigma$)}: $\sigma = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_i - \mu)^2}$
    \item \textbf{Minimum and Maximum}: Range of values
    \item \textbf{Skewness}: $\text{Skew} = \frac{1}{n}\sum_{i=1}^{n}\left(\frac{x_i - \mu}{\sigma}\right)^3$
    \item \textbf{Kurtosis}: $\text{Kurt} = \frac{1}{n}\sum_{i=1}^{n}\left(\frac{x_i - \mu}{\sigma}\right)^4 - 3$
    \item \textbf{Coefficient of Variation}: $\text{CV\%} = \frac{\sigma}{\mu} \times 100\%$
\end{itemize}

\textbf{Effectiveness:}

\textbf{Skewness} measures distribution asymmetry. Skewness = 0 indicates symmetry (like a normal distribution). Positive skewness means a long right tail (many small values, few large ones), while negative skewness indicates a long left tail. Highly skewed features might benefit from transformation.

\textbf{Kurtosis} measures tail heaviness. High kurtosis indicates heavy tails with potential outliers, while low kurtosis suggests a flatter distribution. This helps identify data quality issues.

\textbf{Coefficient of Variation (CV\%)} provides relative variability, allowing fair comparison between features with different scales. Features with high CV\% show high relative variability and may be strong predictors.

\subsubsection{Preprocessing Methods}

\paragraph{Stratified Train-Test Split}

\textbf{Defintition} We divided the dataset into training and test sets using stratified sampling, which maintains the same class proportions in both subsets as in the original dataset.

For a dataset with $C$ classes, if class $i$ represents proportion $p_i$ of the total data, stratified splitting ensures:
$$p_i^{\text{train}} = p_i^{\text{test}} = p_i^{\text{original}}$$

We used a 70/30 split ratio:
\begin{itemize}
    \item Training: 70\% of data
    \item Test: 30\% of data
    \item Random state: 42 (for reproducibility)
\end{itemize}

 Standard random splitting can be problematic for imbalanced datasets. With rare classes, random splitting might accidentally place most or all instances of a rare class in one set, making training or evaluation unreliable or impossible.

Stratified splitting guarantees proportional representation of all classes in both sets, ensuring:
\begin{itemize}
    \item Every class appears in training data for learning
    \item Every class appears in test data for evaluation
    \item Training and test distributions match the real-world distribution
    \item Reliable performance estimates across all classes
\end{itemize}

\paragraph{Standard Scaling}

\textbf{Definition} Standard scaling (z-score normalization) transforms each feature to have mean 0 and standard deviation 1 using:

$$x_{\text{scaled}} = \frac{x - \mu}{\sigma}$$

where $\mu$ is the feature mean and $\sigma$ is the standard deviation.

\textbf{Critical implementation}: To prevent data leakage, we must:
\begin{enumerate}
    \item Fit the scaler on training data only (calculate $\mu$ and $\sigma$ from training)
    \item Transform training data using these parameters
    \item Transform test data using the same parameters from step 1
\end{enumerate}

 Features often have vastly different scales (e.g., elevation in thousands of meters vs. binary 0/1 indicators). Without scaling:

\begin{itemize}
    \item \textbf{Distance-based algorithms} (KNN, SVM) are dominated by high variance features because distance calculations treat all features equally
    \item \textbf{Gradient-based optimization} (neural networks, logistic regression) may converge slowly or poorly
    \item \textbf{Feature importance} becomes difficult to interpret when scales differ
\end{itemize}

We chose StandardScaler over alternatives because:
\begin{itemize}
    \item \textbf{vs. Min-Max Scaling}: More robust to outliers; doesn't compress data into fixed [0,1] range
    \item \textbf{vs. No Scaling}: Essential for KNN and SVM; good practice even for tree based models
    \item \textbf{vs. Robust Scaler}: Standard scaler is simpler and works well when data is roughly normal
\end{itemize}

StandardScaler maintains the relative distribution shape while ensuring features contribute equally to distance calculations.

\subsection{Class Imbalance Handling} % Lam SMOTE voi Class Weight nha Huy


\subsection{Model Selection}


\subsection{Evaluation Metrics and Computational consideration} % de t lam phan nay cho nha huy