\section{Methodology}
\subsection{Class Imbalance Handling}

One of the most significant challenges in this dataset is the severe imbalance across the seven cover type classes. As shown in the class distribution before any resampling, Type 2 (Lodgepole Pine) dominates the training set with 198,310 samples while Type 4 (Cottonwood/Willow) contains only 1,923 with a ratio of over 100:1. Left unaddressed, this imbalance would cause most models to develop a strong bias toward the majority classes, achieving high overall accuracy while almost entirely failing to recognize the rarer cover types.

To investigate the actual impact of class imbalance on model behavior, we designed two experimental scenarios that run in parallel throughout the study: one trained on the original imbalanced data, and one trained on resampled data using SMOTE. In addition, class weighting was applied as a complementary strategy during model training. Each of these approaches is described in the subsections below.

\subsubsection{SMOTE (Synthetic Minority Oversampling Technique)}

\textbf{Without SMOTE} serves as our baseline scenario. All four models are trained directly on the original 406,708 training samples with their natural class distribution intact. This scenario reflects real-world conditions where collecting additional labeled data is not feasible, and it allows us to observe how each model's inherent algorithm handles skewed class frequencies on its own.

\textbf{With SMOTE} is our resampling scenario. Unlike simple random oversampling which merely duplicates existing minority samples, SMOTE generates entirely new synthetic samples by interpolating between existing minority class instances in feature space. The core intuition is that real minority samples and their nearest neighbors should belong to the same class, so any point constructed between them is a reasonable synthetic representative of that class.

The SMOTE algorithm operates through the following steps for each minority class:

\textbf{Step 1: Identify minority samples.}
For each class that needs to be upsampled, SMOTE isolates all existing instances belonging to that class.

\textbf{Step 2: Find \textit{k} nearest neighbors.}
For each minority sample $\mathbf{x}_i$, SMOTE computes its $k$ nearest neighbors within the same class using Euclidean distance in the feature space. In our implementation, $k = 5$ neighbors were used, which is the standard default.

\textbf{Step 3: Select a random neighbor.}
One neighbor $\mathbf{x}_{zi}$ is randomly chosen from the $k$ nearest neighbors of $\mathbf{x}_i$.

\textbf{Step 4: Generate a synthetic sample.}
A new synthetic instance $\mathbf{x}_{new}$ is created by interpolating between $\mathbf{x}_i$ and $\mathbf{x}_{zi}$ according to the following formula:

\begin{equation}
  \mathbf{x}_{new} = \mathbf{x}_i + \lambda \times (\mathbf{x}_{zi} - \mathbf{x}_i)
\end{equation}

\noindent where $\lambda$ is a random number drawn uniformly from the interval $[0, 1]$. This places the synthetic sample at a random point along the line segment connecting the two neighbors in feature space, ensuring the generated instance is not simply a copy of either parent but a genuinely new point within the local data manifold of the minority class.

\textbf{Step 5: Repeat until balanced.}
Steps 2 through 4 are repeated across all minority samples until the target class size is reached. In our case, the target was set to match the majority class size of 198,310 samples, producing a perfectly balanced training set.

One important property of this approach is that SMOTE operates entirely in feature space rather than input space, meaning it is applied after numerical encoding and standardization of the features. It is also strictly applied only to the training set and the test set retains the original imbalanced distribution throughout to ensure that our evaluation metrics reflect true generalization to real-world conditions and are not inflated by artificially balanced test data.

The effect of SMOTE on our training data was substantial. The original distribution of 406,708 samples was expanded to 1,388,170 samples with every class balanced at 198,310 instances, as summarized in Table~\ref{tab:smote_distribution}.

\begin{table}[h]
  \centering
  \caption{Class distribution before and after SMOTE}
  \label{tab:smote_distribution}
  \begin{tabular}{llrr}
    \toprule
    \textbf{Type}  & \textbf{Cover Type} & \textbf{Before SMOTE} & \textbf{After SMOTE} \\
    \midrule
    Type 1         & Spruce/Fir          & 148,288               & 198,310              \\
    Type 2         & Lodgepole Pine      & 198,310               & 198,310              \\
    Type 3         & Ponderosa Pine      & 25,028                & 198,310              \\
    Type 4         & Cottonwood/Willow   & 1,923                 & 198,310              \\
    Type 5         & Aspen               & 6,645                 & 198,310              \\
    Type 6         & Douglas-fir         & 12,157                & 198,310              \\
    Type 7         & Krummholz           & 14,357                & 198,310              \\
    \midrule
    \textbf{Total} &                     & \textbf{406,708}      & \textbf{1,388,170}   \\
    \bottomrule
  \end{tabular}
\end{table}

\noindent It is worth noting that while perfect class balance is theoretically appealing, generating over 980,000 synthetic samples introduces a significant computational burden, particularly for models that scale poorly with training set size. This trade-off between minority class fairness and practical feasibility is a central tension explored throughout our experiments.

\subsubsection{Class Weighting}

In addition to SMOTE, class weighting was applied as a lightweight complementary strategy to further address class imbalance during model training. For Decision Tree, Random Forest, and SVM, the \texttt{class\_weight=`balanced'} parameter was enabled. This instructs the model to inversely weight each class by its frequency, penalizing misclassification of minority classes more heavily during the optimization process. Concretely, the weight assigned to each class $c$ is computed as:

\begin{equation}
  w_c = \frac{n}{k \times n_c}
\end{equation}

\noindent where $n$ is the total number of training samples, $k$ is the number of classes, and $n_c$ is the number of samples in class $c$. Under this scheme, a rare class like Type 4 with only 1,923 samples receives a weight approximately 103 times higher than Type 2, effectively forcing the model to treat each minority misclassification as far more costly during learning.

Unlike SMOTE, class weighting does not alter the training data or increase its size and it only adjusts the internal loss function of the model. This makes it computationally free and easy to apply without any preprocessing overhead. It is therefore used in both the Without SMOTE and With SMOTE scenarios as a consistent baseline correction across applicable models.

Note that KNN does not support class weighting natively due to its non-parametric, distance-based prediction mechanism, where classification is determined purely by neighbor votes with no loss function to reweight. Accordingly, no class weighting was applied for KNN in any experimental scenario.

\newpage